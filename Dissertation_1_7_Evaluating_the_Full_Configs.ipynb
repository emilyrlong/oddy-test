{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Dissertation 1.7: Evaluating the Full Configs",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vPs64QA1Zdov",
        "21tUtyyVrkIt",
        "ZYFh_Qa6Ssxd",
        "dNNhFwP4y49P",
        "CzeV2O4Szj2W",
        "yVeDQaSdrkJC",
        "LgG_YT7UrkJQ",
        "0k6oFoxTrkJi",
        "IogyryF2lFBL",
        "HyzSGUDsrkI7",
        "Dqb_yjAo3cO_",
        "VTvj7Q0oYk75",
        "HwAi3yImUYln",
        "EdfVgnDYUgvN",
        "Uv36tyEv9ptP",
        "UC_jfjOQ_Fz1"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilyrlong/oddy-test/blob/main/Dissertation_1_7_Evaluating_the_Full_Configs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Dissertation 1.7: Evaluating the Full Configs\n",
        "\n",
        "A notebook to run in parallel with dissertation 1.6. As it trains the model, this notebook will evaluate on the validation set. This code uses the [tutorial](https://neptune.ai/blog/how-to-train-your-own-object-detector-using-tensorflow-object-detection-api) from Anton Margonuv at Neptune.ai. In previous code, we were only utilising a part of the model configs, but we want to use a lot more of the in-built training, validation, and testing features.\n",
        "\n",
        "Make sure the runtime type is on GPU + Standard RAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m35sE9VxMfbf"
      },
      "source": [
        "# Connect colab to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hy3pzP95-eE"
      },
      "source": [
        "# **Part 1:** Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## **Step 1**: Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAxXN6EYMKFZ"
      },
      "source": [
        "# !pip install tensorflow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZYLovMlYNp6"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-J7whqrkIl"
      },
      "source": [
        "Install the Tensorflow 2 [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "source": [
        "# uncomment the next line if you want to delete an existing models directory\n",
        "!rm -rf ./models/\n",
        "\n",
        "# clone the Tensorflow Model Garden\n",
        "!git clone --depth 1 https://github.com/tensorflow/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "source": [
        "# install the Object Detection API\n",
        "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UTKCOjtR6SR"
      },
      "source": [
        "# Testing the installation of the object detection API\n",
        "# !python models/research/object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0GEYMBUQrum"
      },
      "source": [
        "# Installing the COCO API:\n",
        "# !pip install cython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT3V-chbQ-zt"
      },
      "source": [
        "# Cloning COCO API\n",
        "!git clone https://github.com/cocodataset/cocoapi.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kkLY3yuRUSN"
      },
      "source": [
        "# Copying the python tools into the research folder\n",
        "%cp -r cocoapi/PythonAPI/pycocotools ./models/research/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tUtyyVrkIt"
      },
      "source": [
        "## **Step 2**: Import Packages\n",
        "\n",
        "Let's now import the packages you will use in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW7YShaj2iXN"
      },
      "source": [
        "# !pip install dfply"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZcqD4NLdnf4"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "# from dfply import *\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-auwvBQrkIw"
      },
      "source": [
        "### **Step 2.1**: Import Object Detection API packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YihOFxxrkIw"
      },
      "source": [
        "# import the label map utility module\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "# import module for reading and updating configuration files.\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "# import module for visualization. use the alias `viz_utils`\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "# import module for building the detection model\n",
        "from object_detection.builders import model_builder\n",
        "### END CODE HERE ###\n",
        "\n",
        "# import module for utilities in Colab\n",
        "from object_detection.utils import colab_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFh_Qa6Ssxd"
      },
      "source": [
        "## **Step 3**: Setting Up Workspace and Adding Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "megFtwkfSq_F"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNKRzPxQS2PZ"
      },
      "source": [
        "# Making workspace and data directories\n",
        "%mkdir workspace\n",
        "%mkdir workspace/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhBAFevfTN-8"
      },
      "source": [
        "# Copying data TFRecords from Google Drive\n",
        "%cp /content/drive/MyDrive/Dissertation/TFRecords/test.record workspace/data\n",
        "%cp /content/drive/MyDrive/Dissertation/TFRecords/val.record workspace/data\n",
        "%cp /content/drive/MyDrive/Dissertation/TFRecords/train.record workspace/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-lukHv6ULW4"
      },
      "source": [
        "# Copy the label map to the data folder\n",
        "%cp /content/drive/MyDrive/Dissertation/labels/label_map.pbtxt workspace/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnER2o43V1aK"
      },
      "source": [
        "##**Step 4**: Downloading the Pre-Trained Models\n",
        "Copy the link from the model that you want in the TensorFlow 2 Detection [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cc71AwZWhVD"
      },
      "source": [
        "# Making a directory for the pre-trained models\n",
        "%mkdir workspace/pre_trained_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYWQ4UymW3BF"
      },
      "source": [
        "# Change to the pre_trained_models directory\n",
        "%cd workspace/pre_trained_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYXNqDzHV0_L"
      },
      "source": [
        "# Paste the link to the desired model here: ex. RetinaNet (SSD + ResNet50)\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "    \n",
        "# untar (decompress) the tar file\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "\n",
        "# copy the checkpoint to the test_data folder models/research/object_detection/test_data/\n",
        "# !mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlYmiIDnXFNr"
      },
      "source": [
        "# Download the Faster R-CNN V1 Resnet 50, 640x640 checkpoint\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz\n",
        "    \n",
        "# untar (decompress) the tar file\n",
        "!tar -xf faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh0mgDaUELgk"
      },
      "source": [
        "# Download the CenterNet HourGlass104 512 x 512 model - highest mAP of the small image models \n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200713/centernet_hg104_512x512_coco17_tpu-8.tar.gz\n",
        "    \n",
        "# untar (decompress) the tar file\n",
        "!tar -xf centernet_hg104_512x512_coco17_tpu-8.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUGRa2bo1mU4"
      },
      "source": [
        "# Download the EfficientDet D1 640x640 model - higher mAP than RetinaNet, faster than CenterNet \n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d1_coco17_tpu-32.tar.gz\n",
        "    \n",
        "# untar (decompress) the tar file\n",
        "!tar -xf efficientdet_d1_coco17_tpu-32.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYpFlg5QYWwR"
      },
      "source": [
        "## **Step 5**: Creating Directories for Customised Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLqZBwf_b_9D"
      },
      "source": [
        "Instead of setting up a folder here, I've made new folders in my Google Drive and added the pipeline config that I worked on before at ```/content/drive/MyDrive/Dissertation/models_workspace/eff_det/v1/eff_det_d1_pipeline.config```. \n",
        "\n",
        "Update the config:\n",
        "1. **num_classes**: 9\n",
        "2. **batch_size** (train_config): 10\n",
        "3. **batch_size** (eval_config): 1\n",
        "4. **fine_tune_checkpoint**: path to downloaded checkpoint\n",
        "5. **fine_tune_checkpoint_type**: ‘detection’\n",
        "6. **use_bfloat16**: false\n",
        "7. **label_map_path**: path to label_map.pbtxt (both in train_input_reader and eval_input_reader)\n",
        "8. (train_input_reader) **input_path**: path to train.record \n",
        "9. (eval_input_reader) **input_path**: path to val.record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stttbwxr60Qw"
      },
      "source": [
        "## **Step 8**: Evaluate the Model on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaI-zYoRNl0O"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkwoRXVkolE3"
      },
      "source": [
        "# Evaluate the model on validation data\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "  --pipeline_config_path=/content/drive/MyDrive/Dissertation/models_workspace/eff_det/v3/eff_det_d1_pipeline_5.config \\\n",
        "  --model_dir=/content/drive/MyDrive/Dissertation/models_workspace/eff_det/v3 \\\n",
        "  --checkpoint_dir=/content/drive/MyDrive/Dissertation/models_workspace/eff_det/v3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj8y1dA1RSnj"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i3QLwpnQBQo"
      },
      "source": [
        "%tensorboard --logdir=/content/drive/MyDrive/Dissertation/models_workspace/eff_det"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBtLNUwlSBOH"
      },
      "source": [
        "!tensorboard dev upload \\\n",
        "  --logdir /content/drive/MyDrive/Dissertation/models_workspace/eff_det/v3 \\\n",
        "  --name \"EfficientDet D1 - Config 4 - 100 Epochs\" \\\n",
        "  --description \"Training and Validation Data for Oddy Tests\" \\\n",
        "  --one_shot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I6_kh1AJmLc"
      },
      "source": [
        "# Test data evaluation\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "  --pipeline_config_path=/content/drive/MyDrive/Dissertation/models_workspace/eff_det/v1/eff_det_d1_pipeline_v1_test.config \\\n",
        "  --model_dir=/content/drive/MyDrive/Dissertation/models_workspace/eff_det/v1 \\\n",
        "  --checkpoint_dir=/content/drive/MyDrive/Dissertation/models_workspace/eff_det/v1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2BzFZWQKFUD"
      },
      "source": [
        "%tensorboard --logdir=/content/drive/MyDrive/Dissertation/models_workspace/eff_det/v1/eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4sOtNINiBc4"
      },
      "source": [
        "# Validation check on CenterNet\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "  --pipeline_config_path=/content/drive/MyDrive/Dissertation/saved_models/CenterNet-20E-Aug11/centernet/pipeline_test.config \\\n",
        "  --model_dir=/content/drive/MyDrive/Dissertation/saved_models/CenterNet-20E-Aug11/centernet \\\n",
        "  --checkpoint_dir=/content/drive/MyDrive/Dissertation/saved_models/CenterNet-20E-Aug11/centernet/checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0BVgPo2jqiZ"
      },
      "source": [
        "# Validation check on RetinaNet\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "  --pipeline_config_path=/content/drive/MyDrive/Dissertation/saved_models/RetinaNet-50E-Aug11/ssd/pipeline.config \\\n",
        "  --model_dir=/content/drive/MyDrive/Dissertation/saved_models/RetinaNet-50E-Aug11/ssd \\\n",
        "  --checkpoint_dir=/content/drive/MyDrive/Dissertation/saved_models/RetinaNet-50E-Aug11/ssd/checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FvB6aGGqyOI"
      },
      "source": [
        "# Validation data for final checkpoint of EfficientDet\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "  --pipeline_config_path=/content/drive/MyDrive/Dissertation/saved_models/EfficientDet-D1-V3-Aug15/pipeline.config \\\n",
        "  --model_dir=/content/drive/MyDrive/Dissertation/saved_models/EfficientDet-D1-V3-Aug15 \\\n",
        "  --checkpoint_dir=/content/drive/MyDrive/Dissertation/saved_models/EfficientDet-D1-V3-Aug15/checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNNhFwP4y49P"
      },
      "source": [
        "## **Step 7**: Tensorboard to Visualise the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oiAG4qc0Srk"
      },
      "source": [
        "# If you need to move any files\n",
        "# !mv /content/workspace/models/faster_rcnn/v1/ckpt* /content/workspace/models/ssd/v1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkbIFNIa3iqf"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKXsF-ze1HRD"
      },
      "source": [
        "# %tensorboard --logdir=/content/workspace/models/centernet/v1\n",
        "%tensorboard --logdir=/content/workspace/models/eff_det/v1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA5eY9-nyv5T"
      },
      "source": [
        "## **Step 9**: Export Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DemiXcOKV_qk"
      },
      "source": [
        "# Copying the config only\n",
        "%cp -r /content/workspace/models/eff_det/v1/pipeline.config /content/drive/MyDrive/Dissertation/configs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4W0pTQxzGgk"
      },
      "source": [
        "# Copy the export python file to train the model into the workspace\n",
        "%cp -r /content/models/research/object_detection/exporter_main_v2.py /content/workspace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMbAMjaAzXKy"
      },
      "source": [
        "# Make new directories for the trained models\n",
        "%mkdir /content/workspace/exported_models\n",
        "%mkdir /content/workspace/exported_models/centernet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5jMLGz1zosv"
      },
      "source": [
        "%cd workspace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls2hWJ4Jzix0"
      },
      "source": [
        "# Run the Python exporter script\n",
        "!python exporter_main_v2.py \\\n",
        "  --pipeline_config_path=/content/workspace/models/centernet/v1/pipeline.config \\\n",
        "  --trained_checkpoint_dir=/content/workspace/models/centernet/v1 \\\n",
        "  --output_directory=/content/workspace/exported_models/centernet/ \\\n",
        "  --input_type=image_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgRpbLCy7udQ"
      },
      "source": [
        "# Save exported model in Google Drive\n",
        "%cp -r /content/workspace/exported_models/centernet /content/drive/MyDrive/Dissertation/saved_models/CenterNet-20E-Aug11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1PVpvVc6C15"
      },
      "source": [
        "# **Part 2**: Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcPfo571mnzz"
      },
      "source": [
        "## **Step 2.1**: Import and Clean Label Data\n",
        "The labeller MakeSense.ai outputted (xmin, ymin, xdiff, ydiff) where xdiff and ydiff are equal to the difference between the minimum and maximum coordinates, so we need to make some new columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOf3xCKvmm_b"
      },
      "source": [
        "# Load in the csv from the labels folder in drive\n",
        "label_df = pd.read_csv('/content/drive/MyDrive/Dissertation/labels/Fulldata_Aug12.csv')\n",
        "# For the numpys, we need the un-resized data\n",
        "# label_df = pd.read_csv('/content/drive/MyDrive/Dissertation/labels/Fulldata_Aug2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA6krYHtnSTl"
      },
      "source": [
        "label_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzeV2O4Szj2W"
      },
      "source": [
        "### **Step 2.1.1**: Getting Integer Class Values\n",
        "We need to make a column with the mapped integer values for the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCjMj0_OsR7n"
      },
      "source": [
        "# Load label map from file\n",
        "# Function found here: https://github.com/tensorflow/models/blob/master/research/object_detection/utils/label_map_util.py\n",
        "label_map = label_map_util.load_labelmap('/content/drive/MyDrive/Dissertation/labels/StringIntLabelMap.pbtxt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qnY4YTNx6JA"
      },
      "source": [
        "# Convert to dictionary\n",
        "label_dict = label_map_util.get_label_map_dict(label_map,use_display_name=True)\n",
        "label_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgu1JEZVzRzx"
      },
      "source": [
        "# Map the label dictionary to a column to populate the corresponding class integer values\n",
        "# https://kanoki.org/2019/04/06/pandas-map-dictionary-values-with-dataframe-columns/\n",
        "label_df['classInt'] = label_df['class'].map(label_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeDQaSdrkJC"
      },
      "source": [
        "### **Step 2.1.2**: Define the category index dictionary + NumClasses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWBqFVMcweF-"
      },
      "source": [
        "# define a dictionary describing the corrosion classes\n",
        "category_index = {\n",
        "    1 : {\n",
        "        'id'  : 1, \n",
        "        'name': 'Ag-P'\n",
        "    },\n",
        "    2 : {\n",
        "        'id'  : 2,\n",
        "        'name': 'Ag-T'\n",
        "    },\n",
        "    3 : {\n",
        "        'id'  : 3,\n",
        "        'name': 'Ag-U'\n",
        "    },\n",
        "    4 : {\n",
        "        'id'  : 4,\n",
        "        'name': 'Cu-P'\n",
        "    },\n",
        "    5 : {\n",
        "        'id'  : 5,\n",
        "        'name': 'Cu-T'\n",
        "    },\n",
        "    6 : {\n",
        "        'id'  : 6,\n",
        "        'name': 'Cu-U'\n",
        "    },\n",
        "    7 : {\n",
        "        'id'  : 7,\n",
        "        'name': 'Pb-P'\n",
        "    },\n",
        "    8 : {\n",
        "        'id'  : 8,\n",
        "        'name': 'Pb-T'\n",
        "    },\n",
        "    9 : {\n",
        "        'id'  : 9,\n",
        "        'name': 'Pb-U'\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOkztZ0-Fytp"
      },
      "source": [
        "# Specify the number of classes that the model will predict\n",
        "num_classes = 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgG_YT7UrkJQ"
      },
      "source": [
        "## **Step 2.2**: Configure the model and load checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3eHyNmxheJa"
      },
      "source": [
        "### **Step 2.2.1**: Read in the configuration file and build model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59sEM6wXheJa"
      },
      "source": [
        "# Clears old models\n",
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "# define the path to the .config file\n",
        "pipeline_config = '/content/workspace/models/eff_det/v1/pipeline.config'\n",
        "# Load the configuration file into a dictionary\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps3AzqBvheJa"
      },
      "source": [
        "# Read in the object stored at the key 'model' of the configs dictionary\n",
        "model_config = configs['model']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfoRTeV_rkJT"
      },
      "source": [
        "# Use the model_builder build function from the config above\n",
        "detection_model = model_builder.build(model_config = model_config, is_training = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFMDfP1WmDQ1"
      },
      "source": [
        "print(type(detection_model))\n",
        "# Expected: <class 'object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch'>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veAUxoDumIZM"
      },
      "source": [
        "# Run this to check the type of detection_model\n",
        "# detection_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6McB9xTheJa"
      },
      "source": [
        "# check the class variables that are in detection_model\n",
        "# vars(detection_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_BtL4ZZKVvV"
      },
      "source": [
        "### **Step 2.2.2**: Restore the checkpoint\n",
        "\n",
        "- checkpoint_path: `models -> research -> object_detection -> test_data -> checkpoint -> ckpt-0`. **IMPORTANT**: Do not set the path to include the `.index` extension in the checkpoint file name. Will cause errors later  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkvDUUarkJg"
      },
      "source": [
        "checkpoint_path = '/content/workspace/models/eff_det/v1/ckpt-4'\n",
        "# checkpoint_path = '/content/workspace/models/ssd/v1/ckpt-10'\n",
        "\n",
        "# Define a checkpoint\n",
        "checkpoint = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "\n",
        "# Restore the checkpoint to the checkpoint path\n",
        "checkpoint.restore(checkpoint_path).expect_partial()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k6oFoxTrkJi"
      },
      "source": [
        "### **Step 2.2.3**: Run a dummy image to generate the model variables\n",
        "\n",
        "Run a dummy image through the model so that variables are created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MANTSP9LrkJi"
      },
      "source": [
        "# use the detection model's `preprocess()` method and pass a dummy image\n",
        "tmp_image, tmp_shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "\n",
        "# run a prediction with the preprocessed image and shapes\n",
        "tmp_prediction_dict = detection_model.predict(tmp_image, tmp_shapes)\n",
        "\n",
        "# postprocess the predictions into final detections\n",
        "tmp_detections = detection_model.postprocess(tmp_prediction_dict, tmp_shapes)\n",
        "\n",
        "print('Weights restored!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0KvPP-krkJn"
      },
      "source": [
        "# Test Code:\n",
        "assert len(detection_model.trainable_variables) > 0, \"Please pass in a dummy image to create the trainable variables.\"\n",
        "\n",
        "print(detection_model.weights[0].shape)\n",
        "print(detection_model.weights[231].shape)\n",
        "print(detection_model.weights[462].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## **Step 2.3**: Defining Functions\n",
        "\n",
        "You'll define a couple of utility functions for loading images and plotting detections. This code is provided for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRAEdg1AJGtX"
      },
      "source": [
        "### **Function 1**: `plot_detections`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMNWCNIaFYoV"
      },
      "source": [
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "    \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "    Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "          and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "          this function assumes that the boxes to be plotted are groundtruth\n",
        "          boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "          category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "    \"\"\"\n",
        "    \n",
        "    image_np_with_annotations = image_np.copy()\n",
        "    \n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_annotations,\n",
        "        boxes,\n",
        "        classes,\n",
        "        scores,\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=7,\n",
        "        min_score_thresh=0,\n",
        "        line_thickness = 10)\n",
        "    \n",
        "    if image_name:\n",
        "        plt.imsave(image_name, image_np_with_annotations)\n",
        "    \n",
        "    else:\n",
        "        plt.imshow(image_np_with_annotations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyzSGUDsrkI7"
      },
      "source": [
        "### **Function 2**: `load_npy_set`\n",
        "\n",
        "Get the training images from the Google Drive folder and their file names. The images are quite large, so the step which converts them into numpy arrays will take a while.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0m3IESntTsu"
      },
      "source": [
        "# A FUNCTION FOR LOADING IMAGES\n",
        "def load_npy_set(npy_dir):\n",
        "    \"\"\"Load a folder of numpy arrays corresponding to images.\n",
        "    Args: npy_dir - a path to folder of training, validation, or test images. \n",
        "    Returns: images_np - a list of the numpy array versions of the images\n",
        "    \"\"\"\n",
        "    # Getting list of npy files\n",
        "    files = os.listdir(npy_dir)\n",
        "    # Starting an empty list for the npy arrays\n",
        "    images_np = []\n",
        "    # For loop to add each file (npy array) to the image list\n",
        "    for idx, file in enumerate(files):\n",
        "      npy_path = os.path.join(npy_dir,file)\n",
        "      test_img = np.load(npy_path)\n",
        "      images_np.append(test_img)\n",
        "      if idx % 10 == 0:\n",
        "        print('Loading',str(idx),':',file)\n",
        "    # When finished, print message and return \n",
        "    print('Done Loading!')\n",
        "    return images_np, files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqb_yjAo3cO_"
      },
      "source": [
        "### **Function 3**: `box_lister`\n",
        "Converting the box coordinates and class labels into a list of numpy arrays. These can be visualised on top of the images and further converted into tensors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_pzwo4eHSG2"
      },
      "source": [
        "def box_lister(files):\n",
        "    # Define a list of ground truth boxes\n",
        "    gt_boxes = []\n",
        "    # Define a list of class integers\n",
        "    classes = []\n",
        "    # For loop to iterate over the file names\n",
        "    for file in files:\n",
        "      # Need to change 'npy' extension to 'jpg'\n",
        "      file = file.replace('npy','jpg')\n",
        "      # A smaller dataframe to hold the labels for that particular image \n",
        "      image_labels = label_df[label_df['filename']==file]\n",
        "      # Adding error message for if an image doesn't have any labels\n",
        "      if len(image_labels) == 0:\n",
        "        print('Error: file ' + file + ' has no corresponding labels')\n",
        "        continue\n",
        "      # Image height\n",
        "      height = np.unique(image_labels['height'].to_numpy())[0] \n",
        "      # Image width\n",
        "      width = np.unique(image_labels['width'].to_numpy())[0] \n",
        "      # Box array: (ymin, xmin, ymax, xmax)\n",
        "      box_arr = image_labels[['ymin','xmin','ymax','xmax']].to_numpy()\n",
        "      # Normalizing boxes by width and height\n",
        "      box_arr = np.divide(box_arr, [height,width,height,width])\n",
        "      # Appending new array to box list\n",
        "      gt_boxes.append(box_arr)\n",
        "      # Getting the class integers as an array and adding to list\n",
        "      classes.append(image_labels['classInt'].to_numpy())\n",
        "    return gt_boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTvj7Q0oYk75"
      },
      "source": [
        "### **Function 4:** `data_preprocess`\n",
        "Need some data preprocessing so it is formatted properly for the model:\n",
        "- Convert the class labels to one-hot representations\n",
        "- Convert everything (i.e. train images, gt boxes and class labels) to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxoy8D0phBv3"
      },
      "source": [
        "def data_preprocess(train_images_np, gt_boxes, classes):\n",
        "    # The label_id_offset to shift classes to the zeroth index.\n",
        "    label_id_offset = 1\n",
        "    # List for image tensors\n",
        "    train_image_tensors = []\n",
        "    # lists containing the one-hot encoded classes and ground truth boxes\n",
        "    gt_classes_one_hot_tensors = []\n",
        "    gt_box_tensors = []\n",
        "    # Loop to convert the image numpy arrays, box coordinates, and classes\n",
        "    for (train_image_np, gt_box_np, class_np) in zip(train_images_np, gt_boxes, classes):\n",
        "        # convert training image to tensor, add batch dimension, and add to list\n",
        "        train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(train_image_np, dtype=tf.float32), axis=0))\n",
        "        # convert numpy array to tensor, then add to list\n",
        "        gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "        # apply offset to have zero-indexed ground truth classes\n",
        "        zero_indexed_groundtruth_classes = tf.convert_to_tensor(class_np - label_id_offset)\n",
        "        # do one-hot encoding to ground truth classes\n",
        "        gt_classes_one_hot_tensors.append(tf.one_hot(zero_indexed_groundtruth_classes, num_classes))\n",
        "    print('Done prepping data.')\n",
        "    return train_image_tensors, gt_box_tensors, gt_classes_one_hot_tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwAi3yImUYln"
      },
      "source": [
        "### **Function 5**: `whole_image_prep`\n",
        "This function calls to summarise functions 2 - 4 into one line of code. It loads and converts images, boxes, and classes into multiple formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlYunaX-Rghg"
      },
      "source": [
        "def whole_image_prep(npy_dir):\n",
        "  print('Starting Image Loading:')\n",
        "  # Loading the image numpy arrays into a list and \n",
        "  images_np, files = load_npy_set(npy_dir)\n",
        "  print('Starting box coordinate and class lists:')\n",
        "  # Converting csv box coordinates and classes into numpy arrays and lists\n",
        "  gt_boxes, classes = box_lister(files)\n",
        "  print('Converting images, boxes, and classes to tensors:')\n",
        "  # Preprocessing images, boxes, and classes into (one hot) tensors \n",
        "  image_T, gt_box_T, gt_classes_OHT = data_preprocess(images_np, gt_boxes, classes)\n",
        "  return images_np, files, gt_boxes, classes, image_T, gt_box_T, gt_classes_OHT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29bU_8ysWxC3"
      },
      "source": [
        "### **Function 6**: `plot_image_sample`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBD6l-E4N71y"
      },
      "source": [
        "def plot_image_sample(images_np,gt_boxes,classes):\n",
        "    ''' Function to plot a eight images to double check box placements, etc. \n",
        "    '''\n",
        "    %matplotlib inline\n",
        "    # define the figure size\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    # using the plot_detections function to draw the ground truth boxes\n",
        "    for idx in range(8):\n",
        "        plt.subplot(2, 4, idx+1)\n",
        "        plot_detections(\n",
        "          images_np[idx],\n",
        "          gt_boxes[idx],\n",
        "          classes[idx],\n",
        "          np.ones(classes[idx].shape), # scores set to 1\n",
        "          category_index = category_index,\n",
        "        )\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdfVgnDYUgvN"
      },
      "source": [
        "## **Step 2.4:** Loading and Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L47QiXBuFGHk"
      },
      "source": [
        "# LOADING VALIDATION SET\n",
        "val_dir = '/content/drive/MyDrive/Dissertation/new_val_npy'\n",
        "val_images_np, val_files, val_gt_boxes, val_classes, val_image_T, val_gt_box_T, val_gt_classes_OHT = whole_image_prep(val_dir)\n",
        "# 191 images in 3m 7s, then 4m 49s, then 5m 6s\n",
        "# 220 images at 960 x 640 in 2m 12s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riwqvpD5Sh85"
      },
      "source": [
        "'''\n",
        "# LOADING TEST DATA SET\n",
        "test_npy_dir = '/content/drive/MyDrive/Dissertation/move_to_test'\n",
        "# Use the function load_image_set to load in the test set as a list of numpy arrays\n",
        "test_images_np, test_files, test_gt_boxes, test_classes, test_image_T, test_gt_box_T, test_gt_classes_OHT = whole_image_prep(test_npy_dir)\n",
        "# 190 test data only took 2m 55s to load!\n",
        "# 960 x 640: 380 images took 3m 41s to load and process!\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R81aC93nYg3s"
      },
      "source": [
        "#plot_image_sample(test_images_np,test_gt_boxes,test_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-GIbqkzrkJ_"
      },
      "source": [
        "## **Step 2.5**: Process a test image\n",
        "\n",
        "Define a function that returns the detection boxes, classes, and scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aXt-bodrkKA"
      },
      "source": [
        "# Uncomment this decorator if you want to run inference eagerly\n",
        "@tf.function\n",
        "def detect(input_tensor):\n",
        "    \"\"\"Run detection on an input image.\n",
        "\n",
        "    Args:\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "    Returns:\n",
        "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`).\n",
        "    \"\"\"\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "    # use the detection model's postprocess() method to get the the final detections\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    \n",
        "    return detections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzvMrohqiyZD"
      },
      "source": [
        "You can now loop through the test images and get the detection scores and bounding boxes to overlay in the original image. We will save each result in a `results` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNjTj97RrICr"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "label_id_offset = 1\n",
        "results = {'boxes': [], 'scores': []}\n",
        "\n",
        "# Need to adjust this loop to get better results dictionaries??\n",
        "\n",
        "for i in range(8): # len(val_images_np)\n",
        "    input_tensor = val_image_T[i]\n",
        "    detections = detect(input_tensor)\n",
        "    plt.subplot(2, 4, i+1)\n",
        "    plot_detections(\n",
        "      val_images_np[i],\n",
        "      detections['detection_boxes'][0].numpy()[0:6],\n",
        "      detections['detection_classes'][0].numpy()[0:6].astype(np.uint32) + label_id_offset,\n",
        "      detections['detection_scores'][0].numpy()[0:6],\n",
        "      category_index, \n",
        "      figsize=(30, 40)\n",
        "      )\n",
        "    results['boxes'].append(detections['detection_boxes'][0][0].numpy())\n",
        "    results['scores'].append(detections['detection_scores'][0][0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDLVfUrVBMQ"
      },
      "source": [
        "detections['detection_scores']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv36tyEv9ptP"
      },
      "source": [
        "## **Step 2.6**: Turning predictions into TXT files for mAP calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCWWkYTj9uvX"
      },
      "source": [
        "def pred_txt_lister(npy_dir):\n",
        "    # Getting list of npy files\n",
        "    files = os.listdir(npy_dir)\n",
        "\n",
        "    # For loop to iterate over the file names\n",
        "    for i, file in enumerate(files):\n",
        "      # Run the model on that image\n",
        "      detection_test = detect(val_image_T[i])\n",
        "      # Find the non-zero predicted boxes\n",
        "      pred_boxes = detections['detection_boxes'][0].numpy() \n",
        "      pred_boxes = pred_boxes[~np.all(pred_boxes == 0, axis=1)]\n",
        "\n",
        "      # Need to convert boxes from [0,1] to the proper image scale, i.e. [6000,4000]\n",
        "      # Getting height and width for the file:\n",
        "      file = file.replace('npy','jpg')\n",
        "      image_labels = label_df[label_df['filename']==file]\n",
        "      height = np.unique(image_labels['height'].to_numpy())[0] \n",
        "      width = np.unique(image_labels['width'].to_numpy())[0] \n",
        "      # Multiplying boxes by width and height\n",
        "      pred_boxes = np.multiply(pred_boxes, [height,width,height,width])\n",
        "\n",
        "      # Finding the number of non-zero boxes\n",
        "      num_boxes = pred_boxes.shape[0]\n",
        "      # Getting the predicted classes\n",
        "      class_array = detection_test['detection_classes'][0].numpy().astype('int')\n",
        "      # Adding one to the class integers so they start at 1\n",
        "      class_array = class_array[0:num_boxes] + 1\n",
        "      # Getting the scores\n",
        "      scores_array = detections['detection_scores'][0].numpy()[0:num_boxes]\n",
        "\n",
        "      # Adding all the elements to a dataframe and rearranging columns\n",
        "      box_df = pd.DataFrame(pred_boxes, columns = ['ymin','xmin','ymax','xmax'])\n",
        "      box_df['score'] = scores_array\n",
        "      box_df['class'] = class_array\n",
        "      box_df = box_df[['class','score','xmin','ymin','xmax','ymax']]\n",
        "\n",
        "      # Creating a dictionary to map class integers to strings\n",
        "      reverse_dict = {1:'Ag-P', 2:'Ag-T', 3: 'Ag-U', 4: 'Cu-P', 5: 'Cu-T', \n",
        "                      6: 'Cu-U', 7: 'Pb-P', 8: 'Pb-T', 9: 'Pb-U'}\n",
        "      box_df['class'] = box_df['class'].map(reverse_dict)\n",
        "\n",
        "      # Getting new path for txt file\n",
        "      txt_dir = \"/content/drive/MyDrive/Dissertation/input/detection-results-final-ckpt\"\n",
        "      file = file.replace('jpg','txt')\n",
        "      txt_path = os.path.join(txt_dir,file)\n",
        "      # Saving labels as a txt file\n",
        "      np.savetxt(txt_path, box_df, fmt = \"%s\")\n",
        "      print('Saved file: '+file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9SdwjE1-Nle"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu_585UE925E"
      },
      "source": [
        "npy_dir = '/content/drive/MyDrive/Dissertation/new_val_npy'\n",
        "pred_txt_lister(npy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC_jfjOQ_Fz1"
      },
      "source": [
        "## **Step 2.7**: Saving ground truth TXT files for the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kODY5tGr_ExX"
      },
      "source": [
        "def txt_lister(npy_dir):\n",
        "    # Getting list of npy files\n",
        "    files = os.listdir(npy_dir)\n",
        "    # For loop to iterate over the file names\n",
        "    for file in files:\n",
        "      # Need to change 'npy' extension to 'jpg'\n",
        "      file = file.replace('npy','jpg')\n",
        "      # A smaller dataframe to hold the labels for that particular image \n",
        "      image_labels = label_df[label_df['filename']==file][['class','xmin','ymin','xmax','ymax']]\n",
        "      image_labels = image_labels.to_numpy()\n",
        "      # Adding error message if an image doesn't have any labels\n",
        "      if len(image_labels) == 0:\n",
        "        print('Error: file ' + file + ' has no corresponding labels')\n",
        "        continue\n",
        "      # Getting new path for txt file\n",
        "      txt_dir = \"/content/drive/MyDrive/Dissertation/input/ground-truth\"\n",
        "      file = file.replace('jpg','txt')\n",
        "      txt_path = os.path.join(txt_dir,file)\n",
        "      # Saving labels as a txt file\n",
        "      np.savetxt(txt_path, image_labels, fmt = \"%s\")\n",
        "      print('Saved file: '+file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXO7jXHA_LDo"
      },
      "source": [
        "npy_dir = '/content/drive/MyDrive/Dissertation/new_val_npy'\n",
        "# files = os.listdir(npy_dir)\n",
        "txt_lister(npy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq-jP4Sj-mE-"
      },
      "source": [
        "## **Step 2.8**: Calculating mAP\n",
        "\n",
        "The code below and information on mAP is in this Roboflow [blog](https://blog.roboflow.com/mean-average-precision/) and [Colab notebook](https://colab.research.google.com/drive/1pLvZpz0_Ob0yOQ7hxPhVRT04Cb3FGARb#scrollTo=-78frQ4211c8). The mAP python script is from this GitHub [repo](https://github.com/Cartucho/mAP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOTXUMVd-sZZ"
      },
      "source": [
        "%cd /content/\n",
        "# Cloning from github repo with code\n",
        "!git clone https://github.com/Cartucho/mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yetoXsYhmbSx"
      },
      "source": [
        "# Need to replace the class_list.txt list\n",
        "%cd /content/\n",
        "%cp /content/drive/MyDrive/Dissertation/labels/class_list.txt mAP/scripts/extra/class_list.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WD8gPwj-vkR"
      },
      "source": [
        "# The folder comes with subfolders for ground-truth & detection-results with txt files\n",
        "# so we need to remove them\n",
        "%rm -rf mAP/input/ground-truth/\n",
        "%mkdir mAP/input/ground-truth/\n",
        "%rm -rf mAP/input/detection-results/\n",
        "%mkdir mAP/input/detection-results/\n",
        "# Also removing the optional images folder\n",
        "%rm -rf mAP/input/images-optional/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBNqH0fI-46s"
      },
      "source": [
        "# Copying our own txt files into the new folder\n",
        "%cp /content/drive/MyDrive/Dissertation/input/ground-truth/*txt mAP/input/ground-truth/\n",
        "%cp /content/drive/MyDrive/Dissertation/input/detection-results-final-ckpt/*txt mAP/input/detection-results/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihkZbrJ-A6fb"
      },
      "source": [
        "# If you need to copy over new detection-results txt files\n",
        "%cd /content/\n",
        "%rm -rf mAP/input/detection-results/\n",
        "%mkdir mAP/input/detection-results/\n",
        "%cp /content/drive/MyDrive/Dissertation/input/detection-results-ckpt-test/*txt mAP/input/detection-results/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW6653dpAQVa"
      },
      "source": [
        "# Go into mAP directory\n",
        "%cd mAP/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN8UeOo_ASJC"
      },
      "source": [
        "# Run the main python script to get the mAP values\n",
        "!python main.py -na"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}